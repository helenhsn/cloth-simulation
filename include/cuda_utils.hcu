#ifndef CUDA_UTILS_H
#define CUDA_UTILS_H

#include "glm/glm.hpp"

#include <iostream>


extern inline __device__ __host__ float dot(glm::vec3 a, glm::vec3 b)
{
    return a.x*b.x + a.y*b.y + a.z*b.z;
}


extern __device__ __host__ glm::vec3 normalize(glm::vec3 v)
{
    float res = dot(v, v);
    if (res < 0.0000001) return v;
    return v/sqrt(res);
}
extern inline __device__ __host__ glm::vec3 cross(glm::vec3 a, glm::vec3 b)
{
    return  glm::vec3(a.y*b.z - a.z*b.y, a.z*b.x - a.x*b.z, a.x*b.y - a.y*b.x);
}

extern __host__ void cudaErrorCheck(cudaError_t err)
{
    if (err != cudaSuccess)
        std::cout << "CUDA Error: " << cudaGetErrorString(err) << std::endl << std::flush;
}

extern __host__ void bindBuffer(float * &d_ptr, cudaGraphicsResource_t &res)
{
    ///////////////// execute mapping (mark cuda var as graphics resource and request device ptr to access VBO from CUDA code!)
    size_t memory_size;

    cudaErrorCheck(cudaGraphicsMapResources(1,  &res, 0)); // mapping 1 resource to CUDA's var and synchronizing with stream 0
    cudaErrorCheck(cudaGraphicsResourceGetMappedPointer((void **) &d_ptr, &memory_size, res));
}

extern inline __host__ void unbindBuffer(cudaGraphicsResource_t res)
{
    // unmap buffer objects (vbo, ebo) so that OpenGL can use them in the rendering loop!
    cudaErrorCheck(cudaGraphicsUnmapResources(1, &res, 0));
}

#endif
